---
title: "Getting Design Wins for AI Accelerators "
date: 2020-11-18T09:53:55.957000+00:00
category: news
id: 134
url: getting-design-wins
user_id: 840
showOnFrontPage: 0
views: 0
tags:
redirect_from:
thumbnail: /assets/images/portal/article-images/verticalchart1.jpg
layout: portal/portal-article-view
---

<div><img src="{{ '/assets/images/portal/article-images/verticalchart1.jpg' | relative_url }}"></div><br><p>In this IEE article Codeplay CEO and Founder Andrew Richards talks about his experience in working with processor companies to get design wins with their customers. He explains the steps a customer goes through when evaluating a new processor and how important it is to ensure that customers can transition their software to new architectures.</p><p>"There will be huge numbers of software developers working on the AI software – different departments, companies, suppliers and partners while using continuous development for updates and the next generation of applications. They all need to work together."</p><p>He also talks about the opportunities there are for new architectures that go beyond the GPUs that are currently being used to execute AI and machine learning software.</p><p>"The good news is that there are lots of opportunities. GPUs are designed for a wide range of high-value AI workloads but also balanced for graphics processing. They have to be balanced, a midpoint between those graphics and compute systems to appeal to both sets of developers and customers. To compete with GPUs, it’s possible to choose workloads that GPUs are not well-tuned for:<br><br></p><ul><li>Big-data workloads like recommender networks where compute is less important. You don’t need as many teraflops since many of these networks need more bandwidth. You can tune your processor more for bandwidth and less about compute. Or to have access to very, very large amounts of memory</li><li>Sparse operations, e.g. language networks. GPUs are good at dense operations and ok at sparse operations, but it’s possible to do better with language networks and recommender networks</li><li>Lower-precision, but still high-value networks: e.g. semantic segmentation, which can give efficiency savings</li></ul><div>Then you can design hardware that’s tuned for that workload or that range of workloads."</div><div><br></div><div>Read the full article on <a href="https://www.computer.org/publications/tech-news/research/getting-design-win-for-ai-accelerators">the IEEE website</a>.<br></div>
